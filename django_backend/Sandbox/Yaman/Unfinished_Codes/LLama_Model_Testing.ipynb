{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA RTX 5000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return the number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Should show the GPU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import logging\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: D:/huggingface_cache\n",
      "TRANSFORMERS_CACHE: D:/huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: D:/huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "\n",
    "login(\"#\")\n",
    "# Don't forget to remove the Key when uploading to GitHub\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"D:/huggingface_cache\"\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "transformers.utils.hub.TRANSFORMERS_CACHE = \"D:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [09:43<00:00, 145.76s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../Generate_Paragraphs/Results/extracted_chunks_1024_overlap.json\"  \n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    chunk_data = json.load(file)\n",
    "    \n",
    "qa_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"generation_log.txt\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Tracking counters\n",
    "total_chunks = 0\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "token_Size = \"Default\"\n",
    "questions_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA pairs saved to generated_qa_pairs.json\n",
      "Log file saved to generation_log.txt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='w',  \n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "for doc_name, chunks in chunk_data.items():\n",
    "    qa_results[doc_name] = []\n",
    "\n",
    "    for chunk in chunks[:5]:\n",
    "        total_chunks += 1\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Generate {questions_num} question-answer pairs based on the following text segment. \n",
    "        Return the result in valid JSON format as a list of objects.\n",
    "\n",
    "        Text Segment:\n",
    "        \n",
    "        {chunk}\n",
    "\n",
    "        Response Format:\n",
    "        [\n",
    "            {{\"question\": \"What is ...?\", \"answer\": \"The answer is ...\"}},\n",
    "            {{\"question\": \"How does ... work?\", \"answer\": \"It works by ...\"}}\n",
    "        ]\n",
    "\n",
    "        Question answers should be at least 250 words long.\n",
    "\n",
    "        Do NOT include any explanation or preamble before or after the JSON output.\n",
    "        Return ONLY valid JSON output.\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(**inputs)\n",
    "\n",
    "        generated_tokens = output_tokens[0][len(inputs[\"input_ids\"][0]):]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            qa_pairs = json.loads(generated_text)\n",
    "            if isinstance(qa_pairs, list):\n",
    "                qa_results[doc_name].extend(qa_pairs)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                logging.warning(f\"Invalid JSON object (not a list) in document '{doc_name}'\")\n",
    "                fail_count += 1\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(f\"JSONDecodeError for document '{doc_name}'\")\n",
    "            fail_count += 1\n",
    "\n",
    "# Save the QA results\n",
    "output_file_path = \"generated_qa_pairs.json\"\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(qa_results, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Logging summary\n",
    "end_time = time.time()\n",
    "elapsed_time = timedelta(seconds=end_time - start_time)\n",
    "\n",
    "logging.info(f\"Total chunks processed: {total_chunks}\")\n",
    "logging.info(f\"Successful QA generations: {success_count}\")\n",
    "logging.info(f\"Failed QA generations: {fail_count}\")\n",
    "logging.info(f\"Total execution time: {elapsed_time}\")\n",
    "logging.info(f\"Number of Questions: {questions_num}\")\n",
    "logging.info(f\"Token Size: {token_Size}\")\n",
    "\n",
    "print(f\"QA pairs saved to {output_file_path}\")\n",
    "print(f\"Log file saved to {log_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
