{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA RTX 5000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return the number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Should show the GPU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"D:/huggingface_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: D:/huggingface_cache\n",
      "TRANSFORMERS_CACHE: D:/huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: D:/huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\openfl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"#\")\n",
    "# Don't forget to remove the Key when uploading to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\openfl\\lib\\site-packages\\transformers\\utils\\hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [07:11<00:00, 107.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "        Generate {1} question-answer pairs based on the following text segment. \n",
    "        Return the result in valid JSON format as a list of objects.\n",
    "\n",
    "        Text Segment:\n",
    "        \n",
    "Wikipedia is a free online encyclopedia that anyone can edit, and millions already have.\n",
    "\n",
    "Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, with articles that usually contain numerous links guiding readers to more information.\n",
    "\n",
    "Written collaboratively by volunteers known as Wikipedians, Wikipedia articles can be edited by anyone with Internet access, except in limited cases in which editing is restricted to prevent disruption or vandalism. Since its creation on January 15, 2001, it has grown into the world's largest reference website, attracting over a billion visitors each month. Wikipedia currently has more than sixty-four million articles in more than 300 languages, including 6,969,912 articles in English, with 125,967 active contributors in the past month.\n",
    "\n",
    "Wikipedia's fundamental principles are summarized in its five pillars. While the Wikipedia community has developed many policies and guidelines, new editors do not need to be familiar with them before they start contributing.\n",
    "\n",
    "Anyone can edit Wikipedia's text, data, references, and images. The quality of content is more important than the expertise of who contributes it. Wikipedia's content must conform with its policies, including being verifiable by published reliable sources. Contributions based on personal opinions, beliefs, or personal experiences, unreviewed research, libellous material, and copyright violations are not allowed, and will not remain. Wikipedia's software makes it easy to reverse errors, and experienced editors watch and patrol bad edits.\n",
    "\n",
    "Wikipedia differs from printed references in important ways. Anyone can instantly improve it, add quality information, remove misinformation, and fix errors and vandalism. Since Wikipedia is continually updated, encyclopedic articles on major news events appear within minutes. \n",
    "\n",
    "        Response Format:\n",
    "        [\n",
    "            {{\"question\": \"What is ...?\", \"answer\": \"The answer is ...\"}},\n",
    "            {{\"question\": \"How does ... work?\", \"answer\": \"It works by ...\"}}\n",
    "        ]\n",
    "\n",
    "        Question answers should be at least 250 words long.\n",
    "\n",
    "        Do NOT include any explanation or pre-amble before or after the JSON output.\n",
    "        Return ONLY valid JSON output.  \n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = output_tokens[0][len(inputs[\"input_ids\"][0]):]  \n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would like the questions to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would like the questions to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be in the form of a sentence. I would like the questions and answers to be in English.\\n\\nI would like the questions and answers to be related to the paragraph, and the answers to be concise and to the point. I would also like the questions to be in the form of a question, and the answers to be'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_qa(text, chunk_limit, model=\"gpt-4\", questions_num=5):\n",
    "#     # Set up data preparation model\n",
    "#     model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "#     try:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\") #Use torch.bfloat16 if your GPU supports it.\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         return json.dumps({\"error\": f\"The data preparation model was not properly loaded.\"}, indent=4)\n",
    "\n",
    "#     text_chunks = split_text(text, max_tokens=4000)  # Adjust chunk size\n",
    "#     results = []\n",
    "\n",
    "#     total_chunks = len(text_chunks)\n",
    "\n",
    "#     for i, chunk in enumerate(text_chunks[:chunk_limit]):\n",
    "#         print(f'Total {total_chunks}, finished {i + 1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
